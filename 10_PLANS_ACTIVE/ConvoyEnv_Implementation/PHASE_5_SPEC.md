# Phase 5 Specification: Reward Function

**Generated by:** PLANNER Agent
**Date:** January 10, 2026
**For:** BUILDER Agent
**Status:** Ready for Implementation

---

## Status Update

| Phase | Tests | Status |
|-------|-------|--------|
| Phase 0-4 | 46/46 | COMPLETE |
| **Phase 5** | 0/12 | **NOW** |
| Total | 46/80 | 57.5% |

---

## Phase 5 Objective

Implement `RewardCalculator` class that computes step rewards based on:
1. **Safety** - Distance to lead vehicle
2. **Comfort** - Harshness of braking
3. **Appropriateness** - Action matches situation

---

## Prerequisites Verified

| Dependency | Status | Location |
|------------|--------|----------|
| ActionApplicator (returns actual decel) | DONE | `action_applicator.py` |
| ObservationBuilder (provides distance) | DONE | `observation_builder.py` |

---

## Implementation Specification

### RewardCalculator Class

```python
# ml/envs/reward_calculator.py

from typing import Tuple, Dict

class RewardCalculator:
    """
    Calculates reward based on safety, comfort, and appropriateness.

    Reward structure from ARCHITECTURE_V2_MINIMAL_RL.md:
    - Collision: -100
    - Unsafe proximity (<15m): -5
    - Safe following (20-40m): +1
    - Far (>40m): +0.5
    - Harsh braking (>4.5 m/s²): -10
    - Uncomfortable braking (>3.0 m/s²): -2
    - Unnecessary alert (>40m, action>1): -2
    - Missed warning (<15m, action==0): -3
    """

    # Distance thresholds (meters)
    COLLISION_DIST = 5.0
    UNSAFE_DIST = 15.0
    SAFE_DIST_MIN = 20.0
    SAFE_DIST_MAX = 40.0

    # Reward values
    REWARD_COLLISION = -100.0
    REWARD_UNSAFE = -5.0
    REWARD_SAFE = +1.0
    REWARD_FAR = +0.5

    # Comfort thresholds
    HARSH_BRAKE_THRESHOLD = 4.5  # m/s²
    UNCOMFORTABLE_BRAKE_THRESHOLD = 3.0  # m/s²
    PENALTY_HARSH_BRAKE = -10.0
    PENALTY_UNCOMFORTABLE = -2.0

    # Appropriateness penalties
    PENALTY_UNNECESSARY_ALERT = -2.0
    PENALTY_MISSED_WARNING = -3.0

    def _safety_reward(self, distance: float) -> float:
        """
        Calculate safety component of reward.

        Args:
            distance: Distance to nearest lead vehicle in meters

        Returns:
            Safety reward component
        """
        if distance < self.COLLISION_DIST:
            return self.REWARD_COLLISION
        elif distance < self.UNSAFE_DIST:
            return self.REWARD_UNSAFE
        elif self.SAFE_DIST_MIN <= distance <= self.SAFE_DIST_MAX:
            return self.REWARD_SAFE
        elif distance > self.SAFE_DIST_MAX:
            return self.REWARD_FAR
        else:
            # Between 15m and 20m - neutral zone
            return 0.0

    def _comfort_penalty(self, deceleration: float) -> float:
        """
        Calculate comfort penalty based on braking harshness.

        Args:
            deceleration: Actual deceleration applied (positive = braking)

        Returns:
            Comfort penalty (negative or zero)
        """
        decel_abs = abs(deceleration)
        if decel_abs > self.HARSH_BRAKE_THRESHOLD:
            return self.PENALTY_HARSH_BRAKE
        elif decel_abs > self.UNCOMFORTABLE_BRAKE_THRESHOLD:
            return self.PENALTY_UNCOMFORTABLE
        else:
            return 0.0

    def _appropriateness_reward(self, distance: float, action: int) -> float:
        """
        Calculate appropriateness of action given situation.

        Args:
            distance: Distance to lead vehicle
            action: Action taken (0=Maintain, 1=Caution, 2=Brake, 3=Emergency)

        Returns:
            Appropriateness penalty (negative or zero)
        """
        # Unnecessary alert: far away but braking hard
        if distance > self.SAFE_DIST_MAX and action > 1:
            return self.PENALTY_UNNECESSARY_ALERT

        # Missed warning: close but not responding
        if distance < self.UNSAFE_DIST and action == 0:
            return self.PENALTY_MISSED_WARNING

        return 0.0

    def calculate(self, distance: float, action: int,
                  deceleration: float) -> Tuple[float, Dict]:
        """
        Calculate total reward for current step.

        Args:
            distance: Distance to nearest lead vehicle (meters)
            action: Action taken (0-3)
            deceleration: Actual deceleration applied (m/s)

        Returns:
            Tuple of (total_reward, info_dict)
            info_dict contains component breakdown for debugging
        """
        safety = self._safety_reward(distance)
        comfort = self._comfort_penalty(deceleration)
        appropriateness = self._appropriateness_reward(distance, action)

        total = safety + comfort + appropriateness

        info = {
            'reward_safety': safety,
            'reward_comfort': comfort,
            'reward_appropriateness': appropriateness,
            'reward_total': total,
            'distance': distance,
            'action': action,
            'deceleration': deceleration,
        }

        return total, info
```

---

## Test Specification

### Test File: `ml/tests/unit/test_reward_calculator.py`

```python
import pytest
from envs.reward_calculator import RewardCalculator

# === Safety Rewards (4 tests) ===

def test_reward_collision_distance_under_5m_returns_neg_100():
    """d < 5m -> reward = -100."""
    calc = RewardCalculator()
    reward = calc._safety_reward(distance=4.0)
    assert reward == -100.0

def test_reward_unsafe_distance_under_15m_returns_neg_5():
    """5m <= d < 15m -> reward includes -5."""
    calc = RewardCalculator()
    reward = calc._safety_reward(distance=10.0)
    assert reward == -5.0

def test_reward_safe_distance_20_to_40m_returns_pos_1():
    """20m <= d <= 40m -> reward includes +1."""
    calc = RewardCalculator()
    reward = calc._safety_reward(distance=30.0)
    assert reward == 1.0

def test_reward_far_distance_over_40m_returns_pos_0_5():
    """d > 40m -> reward includes +0.5."""
    calc = RewardCalculator()
    reward = calc._safety_reward(distance=50.0)
    assert reward == 0.5


# === Comfort Penalties (3 tests) ===

def test_reward_harsh_brake_over_4_5_returns_neg_10():
    """|decel| > 4.5 m/s² -> reward includes -10."""
    calc = RewardCalculator()
    penalty = calc._comfort_penalty(deceleration=5.0)
    assert penalty == -10.0

def test_reward_uncomfortable_brake_over_3_returns_neg_2():
    """3.0 < |decel| <= 4.5 -> reward includes -2."""
    calc = RewardCalculator()
    penalty = calc._comfort_penalty(deceleration=3.5)
    assert penalty == -2.0

def test_reward_gentle_brake_under_3_no_penalty():
    """|decel| <= 3.0 -> no comfort penalty."""
    calc = RewardCalculator()
    penalty = calc._comfort_penalty(deceleration=2.0)
    assert penalty == 0.0


# === Appropriateness (3 tests) ===

def test_reward_unnecessary_alert_far_and_braking_returns_neg_2():
    """d > 40m AND action > 1 -> reward includes -2."""
    calc = RewardCalculator()
    penalty = calc._appropriateness_reward(distance=50.0, action=2)
    assert penalty == -2.0

def test_reward_missed_warning_close_and_maintain_returns_neg_3():
    """d < 15m AND action == 0 -> reward includes -3."""
    calc = RewardCalculator()
    penalty = calc._appropriateness_reward(distance=10.0, action=0)
    assert penalty == -3.0

def test_reward_appropriate_action_no_penalty():
    """Correct action for situation -> no appropriateness penalty."""
    calc = RewardCalculator()
    # Safe distance, maintain -> appropriate
    penalty = calc._appropriateness_reward(distance=30.0, action=0)
    assert penalty == 0.0


# === Combined Reward (2 tests) ===

def test_reward_calculate_combines_all_components():
    """Total reward = safety + comfort + appropriateness."""
    calc = RewardCalculator()
    # Safe distance (30m) + gentle brake (1.0) + maintain action
    # Safety: +1, Comfort: 0, Appropriateness: 0 = +1
    total, info = calc.calculate(distance=30.0, action=0, deceleration=1.0)
    assert total == 1.0
    assert info['reward_safety'] == 1.0
    assert info['reward_comfort'] == 0.0
    assert info['reward_appropriateness'] == 0.0

def test_reward_calculate_returns_info_dict():
    """Returns (reward, info) where info has component breakdown."""
    calc = RewardCalculator()
    total, info = calc.calculate(distance=25.0, action=1, deceleration=0.5)

    assert 'reward_safety' in info
    assert 'reward_comfort' in info
    assert 'reward_appropriateness' in info
    assert 'reward_total' in info
    assert 'distance' in info
    assert 'action' in info
    assert 'deceleration' in info
```

---

## Implementation Steps

| Step | Task | Status |
|------|------|--------|
| 5.1 | Write 4 tests for `_safety_reward()` | [ ] |
| 5.2 | **VERIFY RED**: All 4 tests fail | [ ] |
| 5.3 | Implement `_safety_reward()` | [ ] |
| 5.4 | **VERIFY GREEN**: All 4 tests pass | [ ] |
| 5.5 | Write 3 tests for `_comfort_penalty()` | [ ] |
| 5.6 | **VERIFY RED**: All 3 tests fail | [ ] |
| 5.7 | Implement `_comfort_penalty()` | [ ] |
| 5.8 | **VERIFY GREEN**: All 3 tests pass | [ ] |
| 5.9 | Write 3 tests for `_appropriateness_reward()` | [ ] |
| 5.10 | **VERIFY RED**: All 3 tests fail | [ ] |
| 5.11 | Implement `_appropriateness_reward()` | [ ] |
| 5.12 | **VERIFY GREEN**: All 3 tests pass | [ ] |
| 5.13 | Write 2 tests for `calculate()` | [ ] |
| 5.14 | **VERIFY RED**: All 2 tests fail | [ ] |
| 5.15 | Implement `calculate()` | [ ] |
| 5.16 | **VERIFY GREEN**: All 2 tests pass | [ ] |
| 5.17 | **PHASE 5 COMPLETE**: 12/12 tests passing | [ ] |

---

## Critical Notes for BUILDER

### 1. Distance Thresholds

The thresholds define distinct zones:
```
0m ----[COLLISION]---- 5m ----[UNSAFE]---- 15m ----[NEUTRAL]---- 20m ----[SAFE]---- 40m ----[FAR]----> inf
       -100                    -5                   0                    +1              +0.5
```

### 2. Deceleration Sign Convention

`deceleration` from `ActionApplicator.apply()` is the **actual** speed reduction (positive when braking). Use `abs()` for comfort calculation to handle any edge cases.

### 3. Info Dict for Debugging

The `info` dict returned by `calculate()` is crucial for:
- TensorBoard logging during training
- Debugging policy behavior
- Understanding why rewards are high/low

---

## Files to Touch

| File | Changes |
|------|---------|
| `ml/envs/reward_calculator.py` | Full implementation (~80 lines) |
| `ml/tests/unit/test_reward_calculator.py` | NEW - 12 tests |

---

## Exit Criteria

- [ ] All 12 Phase 5 tests pass
- [ ] All 46 previous tests still pass (no regressions)
- [ ] Reward values match spec exactly:
  - Collision: -100
  - Unsafe: -5
  - Safe: +1
  - Far: +0.5
  - Harsh brake: -10
  - Uncomfortable: -2
  - Unnecessary alert: -2
  - Missed warning: -3
- [ ] `calculate()` returns both total and info dict
- [ ] Coverage for RewardCalculator = 100%

---

## Quick Commands

```bash
# Run Phase 5 tests only
pytest roadsense-v2v/ml/tests/unit/test_reward_calculator.py -v

# Run all unit tests (should be 58 after Phase 5)
pytest roadsense-v2v/ml/tests/unit/ -v

# Run with coverage
pytest roadsense-v2v/ml/tests/unit/test_reward_calculator.py --cov=ml/envs/reward_calculator --cov-report=term-missing
```

---

**Ready for BUILDER to proceed with Phase 5.**
